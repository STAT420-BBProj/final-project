---
title: "Final Project"
author: "Mark Berman, Joel Kopp, and Richard Wheeler"
date: "7/23/2018"
output: 
  html_document:
     toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
### function that calculates average percent error using actual wins and predicted wins from test data ###
calc_average_pct_error <- function(model){
  test_predicted_wins <- predict(model, newdata = bbproj_tst)
  abs_dif <- abs(test_predicted_wins - bbproj_tst$W)
  avg_pct_error <- mean(abs_dif/test_predicted_wins) * 100
  rmse <- sqrt(mean(bbproj_tst$W - test_predicted_wins) ^ 2)
  list(predicted_wins = test_predicted_wins, avg_pct_error = avg_pct_error, rmse = rmse)
}
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
### function that plots actual wins versus predicted wins from test data
plot_predicted_wins_versus_actual_wins <- function(model_name, predicted_wins, actual_wins, avg_pct_error, rmse){
  xrange <- c(0,max(actual_wins+10))
  yrange <- c(0,max(predicted_wins+10))
  title <- paste(model_name, " : 2014 - 2016 Baseball Seasons")
  sub <- paste("Average Percent Error: ", round(avg_pct_error,2), " %", "   Test RMSE: ", round(rmse,2))
  plot(predicted_wins ~ actual_wins, pch = 20, col = "grey", cex = 1.5, xlab = "Actual Wins", ylab = "Predicted Wins", main= title, sub=sub, xlim=xrange, ylim=yrange)
  abline(a = 0, b = 1, col = "darkorange")
}
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
### function that plots fitted values versus residuals for training data ###
plot_fitted_versus_residuals <- function(fitted_values, residuals, model_name){
  plot(fitted(bic_model), resid(bic_model), col = "grey", pch = 20,
  xlab = "Fitted", ylab = "Residuals", main = model_name)
  abline(h = 0, col = "darkorange", lwd = 2)
}


```

```{r message=FALSE, warning=FALSE, include=FALSE}
### calculate cross validated RMSE for models using training data
calc_loocv_rmse = function(model) {
sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### load training data and clean it up ###
library(readr)
bbproj_trn <- read_csv("bbproj_trn.csv",na = c("", "NA"))
bbproj_trn$X1 <- NULL
bbproj_trn$lgID <- NULL
bbproj_trn$teamID <- NULL
bbproj_trn$divID <- NULL
bbproj_trn$Rank <- NULL
bbproj_trn$G <- NULL
bbproj_trn$Ghome <- NULL
bbproj_trn$L <- NULL
bbproj_trn$name <- NULL
bbproj_trn$teamIDBR   <- NULL
bbproj_trn$teamIDlahman45 <- NULL
bbproj_trn$teamIDretro <- NULL
bbproj_trn$DivWin <- as.factor(bbproj_trn$DivWin)
bbproj_trn$WCWin <- as.factor(bbproj_trn$WCWin)
bbproj_trn$LgWin <- as.factor(bbproj_trn$LgWin)
bbproj_trn$WSWin <- as.factor(bbproj_trn$WSWin)
bbproj_trn$franchID <- as.factor(bbproj_trn$franchID)
bbproj_trn$park <- as.factor(bbproj_trn$park)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
### load test data and clean it up ###
library(readr)
bbproj_tst <- read_csv("bbproj_tst.csv",na = c("", "NA"))
bbproj_tst$X1 <- NULL
bbproj_tst$lgID <- NULL
bbproj_tst$teamID <- NULL
bbproj_tst$divID <- NULL
bbproj_tst$Rank <- NULL
bbproj_tst$G <- NULL
bbproj_tst$Ghome <- NULL
bbproj_tst$L <- NULL
bbproj_tst$name <- NULL
bbproj_tst$teamIDBR   <- NULL
bbproj_tst$teamIDlahman45 <- NULL
bbproj_tst$teamIDretro <- NULL
bbproj_tst$DivWin <- as.factor(bbproj_tst$DivWin)
bbproj_tst$WCWin <- as.factor(bbproj_tst$WCWin)
bbproj_tst$LgWin <- as.factor(bbproj_tst$LgWin)
bbproj_tst$WSWin <- as.factor(bbproj_tst$WSWin)
bbproj_tst$franchID <- as.factor(bbproj_tst$franchID)
bbproj_tst$park <- as.factor(bbproj_tst$park)
```

***

# Introduction #

Conventional wisdom is that money buys happiness (winning) in Major League Baseball.  However, the advent of "Moneyball" in the early 2000s by the Oakland Athletics, Cleveland Indians, and other teams, has lead to a more analytical approach to determining the make-up of Major League rosters.  

As it turns out, money is not the magic elixir when it comes to assembling a winning Major League Baseball (MLB) team.  The following plot shows that salary does not highly correlate with a winning record.  This is substantiated by the companion single linear regression model and summary statistics that show that salary, while significant, only has a marginal impact on wins by an MLB team. The adjusted *$R^2$* from the simple linear regression -- using training data from 2000 through 2013 -- is low.  Furthermore, the *average percent error* that compares actual wins versus predicted wins from the the test data (2014 through 2016) is high.

```{r echo=FALSE, message=FALSE, warning=FALSE}
options(scipen=10)
xrange <- c(min(bbproj_trn$salary-1000000),max(bbproj_trn$salary+1000000))
yrange <- c(0,max(bbproj_trn$W+10))
plot(bbproj_trn$W ~ bbproj_trn$salary, pch = 20, col = "grey", cex = 1.5, xlab = "Salary (US $)", ylab = "Wins", main= "Actual Salary vs Actual Wins (2000 - 2013)", xlim=xrange, ylim=yrange, type="p")
abline(a = 81, b = 0, col = "darkorange")
#legend("bottomright", c("training observations", "81 WINS"), lty = c(1, 2), lwd = 2, col = c("grey", "darkorange"))
legend("bottomright", 
        cex = 1.0, 
        bty = "n", 
        legend = c("training observations", "81 WINS"), 
        text.col = c("grey", "darkorange"),
        col = c("grey", "darkorange"), 
        pch = c(16,15))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
salary_only_model <- lm(W ~ salary, data = bbproj_trn)
summary(salary_only_model)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(salary_only_model),2)))
```

```{r message=FALSE, message=FALSE, warning=FALSE, include=FALSE}
### average percent error for the salary only model ###
result <- calc_average_pct_error(salary_only_model)
avg_pct_error_salary_only_model <- result$avg_pct_error
predicted_wins_salary_only_model <- result$predicted_wins
rmse_salary_only_model <- result$rmse
```

```{r echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search aic model
plot_predicted_wins_versus_actual_wins("Salary Only Model", predicted_wins_salary_only_model, bbproj_tst$W, avg_pct_error_salary_only_model, rmse_salary_only_model)
```

So what are the factors that move the needle?

We explore two related threads in attempting to identify the factors that have a strong impact on a winning baseball team.

The first thread uses multiple linear regression to identify the factors that influence a team's winning record over the course of a season.  The second thread uses logistic regression to identify the factors that influence a team's ability to win their division.

## Data ##

A few words are in order about where we obtained the data from to perform this analysis.

The source of data is the Sean Lahman baseball archive (http://www.seanlahman.com/baseball-archive/), recognized by the *Society for American Baseball Research* (SABR) as the leading detailed player and team data archive from 1874 through the end of the 2017 Major League Baseball season.  We use team statistics from the years 2000 through 2013 as the training dataset and use data from 2014 through 2016 as the test dataset.  (As a note, we were unable to include the 2017 season in our analysis due to unavailability of salary data.)

### Variables ###

The team-based variables we examine fall into one of three categories: offensive, defensive, or administrative.

#### Administrative ####

- `yearID` - Year
- `franchID` - Franchise, or team name
- `W` - **Wins (multiple linear regression response variable)**
- `DivWin` - **Division winner (Y or N factor- Logistic linear regression response variable)**
- `WCWin` - Wild Card winner (Y or N factor)
- `LgWin` - League champion (Y or N factor)
- `WSWin` - World Series winner (Y or N factor)
- `salary` - Team Salary (U.S dollars not adjusted for inflation)

#### Offensive ####

- `R` - Runs scored
- `AB` - At bats
- `H` - Total hits (including doubles, triples, and home runs)
- `X1B` - Singles
- `X2B` - Doubles
- `X3B` - Triples
- `HR` - Home runs
- `BB` - Base on balls (walks)
- `SO` - Strikeouts
- `SB` - Stolen bases
- `CS` - Caught stealing
- `HBP` - Hit by pitch
- `SF` - Sacrifice flies
- `park` - Name of team's home ballpark (factor)
- `attendance` - Home attendance total
- `BPF` - Three-year park factor for batters
- `GIDP` - Grounded into double plays
- `RBI` - Runs Batted In
- `IBB` - Intentional walks
- `TB` - Total bases
- `SLG` - Slugging percentage
- `OBP` - On-base percentage
- `OPS` - On-base plus slugging percentage
- `BABIP` - Batting average for balls in play
- `RC` - Runs created
- `uBB` - Unintentional walks
- `wOBA` - Weighted on-base average

#### Defensive ####

- `RA` - Total runs allowed
- `ER` - Earned runs allowed
- `ERA` - Earned run average
- `CG` - Complete games
- `SHO` - Shutouts
- `SV` - Saves
- `IPOuts` - Outs pitched
- `HA` - Hits allowed
- `HRA` - Home runs allowed
- `BBA` - Walks allowed
- `SOA` - Strikeouts by pitchers
- `E` - Errors
- `DP` - Double plays
- `FP` - Fielding percentage
- `PPF` - Three-year park factor for pitchers
- `WHIP` - Walks and hits per innings pitched

***

# The Search for Better Multiple Linear Regression Models For Predicting Season Wins #

## Method ##

### Better Candidate Multiple Linear Regression Models ###

```{r}
library(faraway)
scope <- W ~ R + AB + H + X2B + X3B + HR + BB + SO + SB + CS + HBP + SF + RA + ER + ERA + CG + SHO + SV + IPouts + HA  + HRA + BBA + SOA + E + DP + FP + attendance + BPF + PPF + salary + RBI + GIDP + IBB + TB + SLG + OBP + OPS + WHIP + BABIP + RC + X1B + uBB + wOBA
formula <- formula(scope)
start_model <- lm(formula, data= bbproj_trn)
n <- length(resid(start_model))
step_search_start_model <- lm(W ~ 1, data= bbproj_trn)
```

#### Backwards Search: BIC Model ####

```{r}
### Backwards Search: BIC Model
bic_model <- step(start_model,  direction = "backward", k = log(n),trace = 0)
summary(bic_model)
```

#### Backwards Search: AIC Model ####

```{r}
### Backwards Search: AIC Model
aic_model <-  step(start_model,  direction = "backward", trace = 0)
summary(aic_model)

```

#### Step Search: BIC Model ####

```{r}
### Step Search: BIC Model
step_bic_model <- step(step_search_start_model, scope = scope, direction = "both",k = log(n),trace = 0)
summary(step_bic_model)
```


#### Step Search: AIC Model ####

```{r}
### Step Search: AIC Model ### 
step_aic_model <- step(step_search_start_model, scope = scope, direction = "both", trace = 0)
summary(step_aic_model)
```


#### Forward Search: BIC Model ####

** Richard: Include model and summary(model) here **

#### Forward Search: AIC Model ####

** Richard: Include model and summary(model) here **


### Evaluation and Refinement of the Candidate Models ###


#### Evaluation of the Backwards Search: BIC Model ####
```{r}
### Breusch-Pagan Test on Backwards Search - BIC Model
library(lmtest)
bptest(bic_model)
```

```{r}
plot_fitted_versus_residuals(fitted(bic_model), resid(bic_model), "Backwards Search - BIC Model")
```


```{r}
### Shapiro - Wilk Normality Test on Backwards Search - BIC Model ###
shapiro.test(resid(bic_model))
```

```{r}
qqnorm(resid(bic_model), main = "Normal Q-Q Plot, Backwards Search - BIC Model", col = "darkgrey")
qqline(resid(bic_model), col = "dodgerblue", lwd = 2)
```
```{r}
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(bic_model),2)))
```



```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total observations -- Backwards Search: BIC Model
std_resid_bic_model <- rstandard(bic_model)[abs(rstandard(bic_model)) > 2]
is_std_resid_gt_five_percent_bic_model <- length(std_resid_bic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_bic_model,"Outliers Exceed 5% of Obs", "Outliers Do Not Exceed 5% of Obs")
```



```{r}
#### Backwards Search - BIC Model: Unusual Observtions -- Cooks Distance > 4 / n
cd_bic_model <- cooks.distance(bic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_bic_model)))
```

```{r}
bbproj_trn[which(cd_bic_model),]
```

```{r}
### VIF > 5 for Backwards Search: BIC Model Coefficients
library(faraway)
vif_bic_model <- vif(bic_model)
vif_bic_model[which(vif_bic_model > 5)]
```

#### Refining the Backwards Search: BIC Model Due to High Variance Inflation Factors ####

```{r}
library(caret)
bic_model_high_vif_cols <- c("R", "AB", "H", "BB", "SO", "RA", "E", "FP", "BABIP", "RC")
indices_to_drop <- findCorrelation(cor(bbproj_trn[,c(bic_model_high_vif_cols)]), cutoff = 0.6)
vars_to_drop <- bic_model_high_vif_cols[indices_to_drop]
vars_to_drop
```

##### A Better And Smaller Backwards Search: BIC Model #####

```{r}

smaller_bic_model <-  lm(W ~ AB  + BB + SO + SF + RA + CG + SV + IPouts +  BBA + BABIP, data = bbproj_trn)
summary(smaller_bic_model)
```

##### Diagnostics for the Better and Smaller Backwards Search: BIC Model #####


```{r}
library(lmtest)
library(faraway)
vif(smaller_bic_model)
print(paste("Number of coefficients with VIF > 5 : ", sum(vif(smaller_bic_model) > 5)))
bptest(smaller_bic_model)
shapiro.test(resid(smaller_bic_model))
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(smaller_bic_model),2)))
std_resid_bic_model <- rstandard(bic_model)[abs(rstandard(smaller_bic_model)) > 2]
is_std_resid_gt_five_percent_bic_model <- length(std_resid_bic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_bic_model,"Outliers Exceed 5% of Obs", "Outliers Do Not Exceed 5% of Obs")
cd_smaller_bic_model <- cooks.distance(smaller_bic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_smaller_bic_model)))
```


#### Evaluation of the Backwards Search: AIC Model ####


```{r}
### Breusch-Pagan Test on AIC Model
library(lmtest)
bptest(aic_model)
```

```{r}
plot_fitted_versus_residuals(fitted(aic_model), resid(aic_model), "Backwards Search - AIC Model")
```

```{r}
### Shapiro - Wilk Normality Test on Backwards Search - AIC Model ###
shapiro.test(resid(aic_model))
```

```{r}
qqnorm(resid(aic_model), main = "Normal Q-Q Plot, Backwards Search - AIC Model", col = "darkgrey")
qqline(resid(aic_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(aic_model),2)))
```


```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total observations -- Backwards Search: AIC Model
std_resid_aic_model <- rstandard(aic_model)[abs(rstandard(aic_model)) > 2]
is_std_resid_gt_five_percent_aic_model <- length(std_resid_aic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_aic_model,"Outliers Exceed 5% of Obs", "Outliers Do Not Exceed 5% of Obs")
```

```{r}
####  Backwards Search - AIC Model: Unusual Observtions -- Cooks Distance > 4 / n
cd_aic_model <- cooks.distance(aic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_aic_model)))
```

```{r}
bbproj_trn[which(cd_aic_model),]
```

```{r}
### VIF > 5 for Backwards Search: AIC Model Coefficients
library(faraway)
vif_aic_model <- vif(aic_model)
vif_aic_model[which(vif_aic_model > 5)]
```

#### Refining the Backwards Search: AIC Model Due to High Variance Inflation Factors ####


```{r}
library(caret)
aic_model_high_vif_cols <- c("R", "AB", "H", "X2B", "X3B", "HR", "BB", "SO", "SF", "RA", "ER", "IPouts", "E", "FP","BPF", "PPF", "IBB", "OBP", "BABIP", "RC", "wOBA")
indices_to_drop <- findCorrelation(cor(bbproj_trn[,c(aic_model_high_vif_cols)]), cutoff = 0.6)
vars_to_drop <- aic_model_high_vif_cols[indices_to_drop]
vars_to_drop
```

##### A Better And Smaller Backwards Search: AIC Model #####

```{r}
#smaller_aic_model <-  lm(formula = W ~  AB + HR + BB + SO + CS + ER + CG + SHO + SV + IPouts + BBA + FP + GIDP + IBB +  BABIP, data = bbproj)
smaller_aic_model <-  lm(formula = W ~  HR + BB + SO + ER + CG + SHO + SV + IPouts + BBA + FP + GIDP + BABIP, data = bbproj_trn)
summary(smaller_aic_model)
```

##### Diagnostics for the Better and Smaller Backwards Search: AIC Model #####

```{r}
library(lmtest)
library(faraway)
vif(smaller_aic_model)
print(paste("Number of coefficients with VIF > 5 : ", sum(vif(smaller_aic_model) > 5)))
bptest(smaller_aic_model)
shapiro.test(resid(smaller_aic_model))
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(smaller_aic_model),2)))
std_resid_aic_model <- rstandard(smaller_aic_model)[abs(rstandard(smaller_aic_model)) > 2]
is_std_resid_gt_five_percent_aic_model <- length(std_resid_aic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_aic_model,"Outliers Exceed 5% of Obs", "Outliers Do Not Exceed 5% of Obs")
cd_smaller_aic_model <- cooks.distance(smaller_aic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_smaller_bic_model)))
```


#### Evaluation of the Step Search: BIC Model ####

```{r}
### Breusch-Pagan Test on Step BIC Model
bptest(step_bic_model)

```

```{r}
plot_fitted_versus_residuals(fitted(step_bic_model), resid(step_bic_model), "Step Search - BIC Model")
```

```{r}
### Shapiro - Wilk Normality Test on Step Search - BIC Model ###
shapiro.test(resid(step_bic_model))
```

```{r}
qqnorm(resid(step_bic_model), main = "Normal Q-Q Plot, Step Search - BIC Model", col = "darkgrey")
qqline(resid(step_bic_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(step_bic_model),2)))
```


```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total observations -- Step Search: BIC Model
std_resid_step_bic_model <- rstandard(step_bic_model)[abs(rstandard(step_bic_model)) > 2]
is_std_resid_gt_five_percent__step_bic_model <- length(std_resid_step_bic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent__step_bic_model,"Exceeds 5% of Obs", "Does Not Exceed 5% of Obs")
```

```{r}
####  Step Search - BIC Model: Unusual Observtions -- Cooks Distance > 4 / n
cd_step_bic_model <- cooks.distance(step_bic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_step_bic_model)))
```

```{r}
bbproj_trn[which(cd_step_bic_model),]
```

```{r}
### VIF > 5 for Step Search: BIC Model Coefficients
library(faraway)
vif_step_bic_model <- vif(step_bic_model)
vif_step_bic_model[which(vif_step_bic_model > 5)]
```

#### Refining the Step Search: BIC Model Due to High Variance Inflation Factors ####



##### A Better And Smaller Step Search: BIC Model #####

```{r}
smaller_step_bic_model <- lm(formula = W ~ SV + R + RA + SHO + CG + X3B + IPouts + AB + salary, data = bbproj_trn)
summary(smaller_step_bic_model)
```

##### Diagnostics for the Better and Smaller Step Search: BIC Model #####

```{r}
library(lmtest)
library(faraway)
vif(smaller_step_bic_model)
print(paste("Number of coefficients with VIF > 5 : ", sum(vif(smaller_step_bic_model) > 5)))
bptest(smaller_step_bic_model)
shapiro.test(resid(smaller_step_bic_model))
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(smaller_step_bic_model),2)))
std_resid_step_bic_model <- rstandard(smaller_step_bic_model)[abs(rstandard(smaller_step_bic_model)) > 2]
is_std_resid_gt_five_percent_step_bic_model <- length(std_resid_step_bic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_step_bic_model,"Outliers Exceed 5% of Obs", "Outliers Do Not Exceed 5% of Obs")
cd_smaller_step_bic_model <- cooks.distance(smaller_step_bic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_smaller_step_bic_model)))
```

#### Evaluation of the Step Search: AIC Model ####

```{r}
### Breusch-Pagan Test on Step AIC Model
bptest(step_aic_model)
```

```{r}
plot_fitted_versus_residuals(fitted(step_aic_model), resid(step_aic_model), "Step Search - AIC Model")
```

```{r}
### Shapiro - Wilk Normality Test on Step Search - AIC Model ###
shapiro.test(resid(step_aic_model))
```

```{r}
qqnorm(resid(step_aic_model), main = "Normal Q-Q Plot, Step Search - AIC Model", col = "darkgrey")
qqline(resid(step_aic_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(step_aic_model),2)))
```


```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total observations -- Step Search: AIC Model
std_resid_step_aic_model <- rstandard(step_aic_model)[abs(rstandard(step_aic_model)) > 2]
is_std_resid_gt_five_percent__step_aic_model <- length(std_resid_step_aic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent__step_aic_model,"Exceeds 5% of Obs", "Does Not Exceed 5% of Obs")
```


```{r}
####  Step Search - AIC Model: Unusual Observtions -- Cooks Distance > 4 / n
cd_step_aic_model <- cooks.distance(step_aic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_step_aic_model)))
```

```{r}
bbproj_trn[which(cd_step_aic_model),]
```
```{r}
### VIF > 5 for Step Search: AIC Model Coefficients
library(faraway)
vif_step_aic_model <- vif(step_aic_model)
vif_step_aic_model[which(vif_step_aic_model > 5)]
```

#### Refining the Step Search: AIC Model Due to High Variance Inflation Factors ####


##### A Better And Smaller Step Search: AIC Model #####

```{r}
smaller_step_aic_model <- lm(formula = W ~ SV + R + SHO + CG + X3B + IPouts + BBA + HRA + SOA, data = bbproj_trn)
summary(smaller_step_aic_model)
```


##### Diagnostics for the Better and Smaller Step Search: AIC Model #####

```{r}
library(lmtest)
library(faraway)
vif(smaller_step_aic_model)
print(paste("Number of coefficients with VIF > 5 : ",sum(vif(smaller_step_aic_model) > 5)))
bptest(smaller_step_aic_model)
shapiro.test(resid(smaller_step_aic_model))
print(paste("Leave One Out Cross-Validated RMSE: ", round(calc_loocv_rmse(smaller_step_aic_model),2)))
std_resid_step_aic_model <- rstandard(smaller_step_aic_model)[abs(rstandard(smaller_step_aic_model)) > 2]
is_std_resid_gt_five_percent_step_aic_model <- length(std_resid_step_aic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_step_aic_model,"Outliers Exceed 5% of Obs", "Outliers Do Not Exceed 5% of Obs")
cd_smaller_step_aic_model <- cooks.distance(smaller_step_aic_model) > 4 / n
print(paste("Number of Influential Observations: ", sum(cd_smaller_step_aic_model)))
```

#### Evaluation of the Forward Search: BIC Model ####

** Richard:  use same diagrams and approach I did. (Copy and paste my code and modify) **

#### Refining the Forward Search: BIC Model Due to High Variance Inflation Factors ####

** Richard: use same approach I did. (Copy and paste my code and modify)**

##### A Better And Smaller Forward Search: BIC Model #####

** Richard: use same approach I did. (Copy and paste my code and modify).**

##### Diagnostics for the Better and Smaller Forward Search: BIC Model #####

** Richard: use same approach I did. (Copy and paste my code and modify.) **

#### Evaluation of the Forward Search: AIC Model ####

** Richard:  use same diagrams and approach I did. (Copy and paste my code and modify) **

#### Refining the Forward Search: AIC Model Due to High Variance Inflation Factors ####

** Richard: use same approach I did. (Copy and paste my code and modify)**

##### A Better And Smaller Forward Search: AIC Model #####

** Richard: use same approach I did. (Copy and paste my code and modify).**

##### Diagnostics for the Better and Smaller Forward Search: AIC Model #####

** Richard: use same approach I did. (Copy and paste my code and modify.) **


### Validate Model Effectiveness Using Test Data  ###


```{r echo=FALSE, message=FALSE, warning=FALSE}
### average percent error for the smaller backwards search bic model ###
result <- calc_average_pct_error(smaller_bic_model)
avg_pct_error_smaller_bic_model <- result$avg_pct_error
predicted_wins_smaller_bic_model <- result$predicted_wins
rmse_wins_smaller_bic_model <- result$rmse
```



```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search bic model
plot_predicted_wins_versus_actual_wins("Smaller Backwards Search BIC Model", predicted_wins_smaller_bic_model, bbproj_tst$W, avg_pct_error_smaller_bic_model, rmse_wins_smaller_bic_model)
```




```{r message=FALSE, warning=FALSE, include=FALSE}
### average percent error for the smaller backwards search aic model ###
result <- calc_average_pct_error(smaller_aic_model)
avg_pct_error_smaller_aic_model <- result$avg_pct_error
predicted_wins_smaller_aic_model <- result$predicted_wins
rmse_wins_smaller_aic_model <- result$rmse
```



```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search aic model
plot_predicted_wins_versus_actual_wins("Smaller Backwards Search AIC Model", predicted_wins_smaller_aic_model, bbproj_tst$W, avg_pct_error_smaller_aic_model,rmse_wins_smaller_aic_model )
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### average percent error for the smaller step search bic model ###
result <- calc_average_pct_error(smaller_step_bic_model)
avg_pct_error_smaller_step_bic_model <- result$avg_pct_error
predicted_wins_smaller_step_bic_model <- result$predicted_wins
rmse_wins_smaller_step_bic_model <- result$rmse
```



```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search bic model
plot_predicted_wins_versus_actual_wins("Smaller Step Search BIC Model", predicted_wins_smaller_step_bic_model, bbproj_tst$W, avg_pct_error_smaller_step_bic_model, rmse_wins_smaller_step_bic_model)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
### average percent error for the smaller step search aic model ###
result <- calc_average_pct_error(smaller_step_aic_model)
avg_pct_error_smaller_step_aic_model <- result$avg_pct_error
predicted_wins_smaller_step_aic_model <- result$predicted_wins
rmse_smaller_step_aic_model <- result$rmse
```




```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search aic model
plot_predicted_wins_versus_actual_wins("Smaller Step Search AIC Model", predicted_wins_smaller_step_aic_model, bbproj_tst$W, avg_pct_error_smaller_step_aic_model,rmse_smaller_step_aic_model)
```

** Richard: Copy and paste the 2 above code blocks for Plotting predicted vs actual wins for your smaller (best) Forward BIC Model.  Modify as required. **


** Richard: Copy and paste the 2 above code blocks for Plotting predicted vs actual wins for your smaller (best) Forward AIC Model.  Modify as required. **

## Results ## 

### And the Winning Linear Regression Model is... ###

## Discussion ##

***

# Logistic Modeling and Prediction of Division Winners #

## Salary as a Predictor for Division Winners ##

Let's turn our attention to logistic regression and our ability to classify and predict division winners using our team predictor set.  First, let's see how well salary alone can predict pennants.

```{r}
salary_only_model <- glm(DivWin ~ salary, data = bbproj_trn, family = binomial)
(salary_only_summary = summary(salary_only_model))
plot(as.numeric(DivWin) - 1 ~ salary, data = bbproj_trn, 
     pch = 20, 
     main = "Probability of Buying a Division Winner",
     ylab = "Probability of Winning Division", 
     xlab = "Team Salary ($)",  
     xlim = c(0, 3e8))
curve(predict(salary_only_model, data.frame(salary = x), type = "response"), 
      add = TRUE, 
      col = "tomato", 
      lty = 2,
      lwd = 2)
```

Graphically, a team's payroll seems to be moderately influential in predicting their chances of taking home a division crown.  Indeed, our Wald test for `salary` alone yields a p-value of `r salary_only_summary$coefficients["salary", "Pr(>|z|)"]`, allowing us to reject the null hypothesis
($H_0 : \beta_{salary} = 0$) for any reasonable value for $\alpha$.  So what happens when we look at our salary model's misclassification rate?

```{r}
salary_prediction <- ifelse(predict(salary_only_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(prevalence = table(bbproj_tst$DivWin) / nrow(bbproj_tst))
(salary_misclass = mean(salary_prediction != bbproj_tst$DivWin))
```

First, let's examine prevalence of division winners.  We see that `r prevalence[2] * 100`% of the teams were division winners, which makes sense, since there are 6 divisions and 30 MLB teams, therefore you will only have 6 division winners per year.  Our salary model has a misclassification rate of `r salary_misclass`, which is worse than our prevalence.  We would have a better misclassification rate if we simply stated that there are no division winners!  We can certainly do better than this.

## Method ##

### Candidate Logistic Regression Models ###

We will begin our search for a better classifier by setting up an initial logistic regression model contain all of the predictors we would like to evaluate.  Then, we will proceed to eliminate predictors using backwards, forwards, and stepwise AIC and BIC searches.  See Appendix C for a complete evaluation of all of the models.  The following will detail the methodology using our best candidate model.

```{r warning=FALSE}
scope <- DivWin ~ W + R + AB + H + X2B + X3B + HR + BB + SO + SB + CS + HBP + SF + RA + ER + ERA + CG + SHO + SV + IPouts + HA  + HRA + BBA + SOA + E + DP + FP + attendance + BPF + PPF + salary + RBI + GIDP + IBB + TB + SLG + OBP + OPS + WHIP + BABIP + RC + X1B + uBB + wOBA
start_model <- glm(DivWin ~ 1, data = bbproj_trn, family = binomial)
n <- length(resid(start_model))
```

#### Forward Search: BIC Model ####

```{r warning=FALSE}
bic_model <- step(start_model, direction = "forward", scope = scope, k = log(n), trace = 0)
summary(bic_model)
```

Our forward search using BIC yields the following logistic model

\[
\log \bigg(\frac{P[DivWin = 1]}{1 - P[DivWin = 1]} \bigg) = \beta_0 + \beta_W W + \beta_{X2B} X2B
\]

where the log odds of a team winning their division is dependent upon wins and doubles.

##### Diagnostics: BIC Model ####

Next, we will evaluate this model using a confusion matrix and calculating its sensitivity, specificity, and its misclassification rate, starting with a cutoff value of 0.5.

```{r}
bic_prediction <- ifelse(predict(bic_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(bic_misclass = mean(bic_prediction != bbproj_tst$DivWin))
(bic_conf_matrix = table(prediction = bic_prediction, actual = bbproj_tst$DivWin))
(bic_sensitivity = bic_conf_matrix[2, 2] / sum(bic_conf_matrix[, 2]))
(bic_specificity = bic_conf_matrix[1, 1] / sum(bic_conf_matrix[, 1]))
```

Here, we beat the salary only model along with the prevalence with a misclassification rate of `r round(bic_misclass, 3)`.  Our specificity looks great for our forward BIC model at `r round(bic_specificity, 3)`.  Our `r bic_conf_matrix[1, 2]` false negatives could come down, however, with an adjustment to our cutoff value.

#### Find an Optimal Cutoff ####

The following function will loop through a vector of potential cutoffs to isolate one that will produce the smallest misclassification rate with a minimal differential between sensitivity and specificity:

```{r}
opt_logistic_cutoff = function(model, cut_start = 0.01, cut_end = 0.99, plotit = TRUE) {
  # Loop through potential cutoffs from cut_start to cut_end to determine a cutoff that
  # produces the lowest misclassification rate with the smallest delta between sensitivity
  # and specificity
  cutoffs = seq(cut_start, cut_end, by = 0.01)
  sens = rep(0, length(cutoffs))
  spec = rep(0, length(cutoffs))
  misclass = rep(0, length(cutoffs))
  delta = rep(0, length(cutoffs))
  for (i in 1:length(cutoffs)) {
    pred = ifelse(predict(model, bbproj_tst, type = "response") > cutoffs[i], "Y", "N")
    conf_mat = table(prediction = pred, actual = bbproj_tst$DivWin)
    sens[i] = conf_mat[2, 2] / sum(conf_mat[, 2])
    spec[i] = conf_mat[1, 1] / sum(conf_mat[, 1])
    misclass[i] = mean(pred != bbproj_tst$DivWin)
    delta[i] = abs(sens[i] - spec[i])
  }
  
  # Get the indicies of the smallest misclassification rates
  min_misclass = which(misclass == min(misclass))
  
  # Get the smallest delta between sensitivity and specificity at the identified
  # misclassification indicies
  min_delta = min_misclass[which.min(delta[min_misclass])]
  
  # Plot sensitivity, specificity, and misclassification if requested
  if (plotit) {
    plot(sens ~ cutoffs,
         xlab = "Cutoff",
         ylab = "Sensitivity/Specificity",
         main = "Sensitivity and Specificity at Varied Cutoffs",
         col = "tomato",
         type = "b",
         ylim = c(0, 1),
         pch = 20)
    lines(spec ~ cutoffs, 
          col = "darkslategray4", 
          type = "b",
          pch = 20)
    lines(misclass ~ cutoffs, 
          col = "darkorange", 
          type = "b",
          pch = 20)
    abline(v = cutoffs[min_delta], lty = 2)
    legend("topright", c("Sensitivity", "Specificity", "Misclassification"),
           col = c("tomato", "darkslategray4", "darkorange"),
           lwd = 1,
           pch = 20)
  }
  
  # Return the misclassification, sensitivity, and specificity at the optimal
  # cutoff value
  c(cutoff = cutoffs[min_delta],
    misclass = misclass[min_delta], 
    sensitivity = sens[min_delta],
    specificity = spec[min_delta])
}
```

```{r}
(opt_cutoff = opt_logistic_cutoff(bic_model, cut_start = 0.1, cut_end = 0.8))
```

Our routine finds an optimal cutoff value for our forward BIC model of `r opt_cutoff[1]`.  Let's proceed to evaluate the diagnostics manually:

```{r}
bic_prediction_3 <- ifelse(predict(bic_model, bbproj_tst, type = "response") > 0.3, "Y", "N")
(bic_misclass_3 = mean(bic_prediction_3 != bbproj_tst$DivWin))
(bic_conf_matrix_3 = table(prediction = bic_prediction_3, actual = bbproj_tst$DivWin))
(bic_sensitivity_3 = bic_conf_matrix_3[2, 2] / sum(bic_conf_matrix_3[, 2]))
(bic_specificity_3 = bic_conf_matrix_3[1, 1] / sum(bic_conf_matrix_3[, 1]))
```

Our misclassification rate went down with our cutoff adjustment to `r round(bic_misclass_3, 3)`.  Additionally, our sensitivity went up significantly (`r round (bic_sensitivity_3, 3)`) and we only had a modest reduction in specificity (`r round(bic_specificity_3, 3)`).  This model looks to be exceptionally adept at classifying division winners, and `salary` is not even a predictor!

***

## Results ##

### And the Winning Logistic Regression Model is... ###

## Discussion ##

***

# Apendices #

## Appendix A - Multiple Linear Regression Models for Inference and Explanation #

## Appendix B - Multiple Linear Regression Models for Prediction

## Appendix C - Logistic Regression Models

```{r warning=FALSE}
scope <- DivWin ~ W + R + AB + H + X2B + X3B + HR + BB + SO + SB + CS + HBP + SF + RA + ER + ERA + CG + SHO + SV + IPouts + HA  + HRA + BBA + SOA + E + DP + FP + attendance + BPF + PPF + salary + RBI + GIDP + IBB + TB + SLG + OBP + OPS + WHIP + BABIP + RC + X1B + uBB + wOBA
formula <- formula(DivWin ~ 1)
start_model <- glm(formula, data = bbproj_trn, family = binomial)
n <- length(resid(start_model))
```

### Backward Search: BIC Model

```{r warning=FALSE}
formula <- formula(scope)
back_bic_model <- step(glm(formula, data = bbproj_trn, family = binomial), 
                       direction = "backward", k = log(n), trace = 0)
summary(back_bic_model)
back_bic_prediction <- ifelse(predict(back_bic_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(back_bic_misclass = mean(back_bic_prediction != bbproj_tst$DivWin))
(back_bic_conf_matrix = table(prediction = back_bic_prediction, actual = bbproj_tst$DivWin))
(back_bic_sensitivity = back_bic_conf_matrix[2, 2] / sum(back_bic_conf_matrix[, 2]))
(back_bic_specificity = back_bic_conf_matrix[1, 1] / sum(back_bic_conf_matrix[, 1]))
```

```{r}
opt_logistic_cutoff(back_bic_model, cut_start = 0.1, cut_end = 0.8)
```

### Forward Search: BIC Model

```{r}
forw_bic_model <- step(start_model, direction = "forward", scope = scope, k = log(n), trace = 0)
summary(forw_bic_model)
forw_bic_prediction <- ifelse(predict(forw_bic_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(forw_bic_misclass = mean(forw_bic_prediction != bbproj_tst$DivWin))
(forw_bic_conf_matrix = table(prediction = forw_bic_prediction, actual = bbproj_tst$DivWin))
(forw_bic_sensitivity = forw_bic_conf_matrix[2, 2] / sum(forw_bic_conf_matrix[, 2]))
(forw_bic_specificity = forw_bic_conf_matrix[1, 1] / sum(forw_bic_conf_matrix[, 1]))
```

```{r}
opt_logistic_cutoff(forw_bic_model, cut_start = 0.1, cut_end = 0.8)
```

### Stepwise Search: BIC Model

```{r}
step_bic_model <- step(start_model, direction = "both", scope = scope, k = log(n), trace = 0)
summary(step_bic_model)
step_bic_prediction <- ifelse(predict(step_bic_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(step_bic_misclass = mean(step_bic_prediction != bbproj_tst$DivWin))
(step_bic_conf_matrix = table(prediction = step_bic_prediction, actual = bbproj_tst$DivWin))
(step_bic_sensitivity = step_bic_conf_matrix[2, 2] / sum(step_bic_conf_matrix[, 2]))
(step_bic_specificity = step_bic_conf_matrix[1, 1] / sum(step_bic_conf_matrix[, 1]))
```

```{r}
opt_logistic_cutoff(step_bic_model, cut_start = 0.1, cut_end = 0.8)
```

### Backward Search: AIC Model

```{r warning=FALSE}
formula <- formula(scope)
back_aic_model <- step(glm(formula, data = bbproj_trn, family = binomial), 
                       direction = "backward", trace = 0)
summary(back_aic_model)
back_aic_prediction <- ifelse(predict(back_aic_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(back_aic_misclass = mean(back_aic_prediction != bbproj_tst$DivWin))
(back_aic_conf_matrix = table(prediction = back_aic_prediction, actual = bbproj_tst$DivWin))
(back_aic_sensitivity = back_aic_conf_matrix[2, 2] / sum(back_aic_conf_matrix[, 2]))
(back_aic_specificity = back_aic_conf_matrix[1, 1] / sum(back_aic_conf_matrix[, 1]))
```

```{r}
opt_logistic_cutoff(back_aic_model, cut_start = 0.1, cut_end = 0.8)
```

```{r}
back_aic_prediction <- ifelse(predict(back_aic_model, bbproj_tst, type = "response") > 0.3, "Y", "N")
(back_aic_misclass = mean(back_aic_prediction != bbproj_tst$DivWin))
(back_aic_conf_matrix = table(prediction = back_aic_prediction, actual = bbproj_tst$DivWin))
(back_aic_sensitivity = back_aic_conf_matrix[2, 2] / sum(back_aic_conf_matrix[, 2]))
(back_aic_specificity = back_aic_conf_matrix[1, 1] / sum(back_aic_conf_matrix[, 1]))
```

### Forward Search: AIC Model

```{r}
forw_aic_model <- step(start_model, direction = "forward", scope = scope, trace = 0)
summary(forw_aic_model)
forw_aic_prediction <- ifelse(predict(forw_aic_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(forw_aic_misclass = mean(forw_aic_prediction != bbproj_tst$DivWin))
(forw_aic_conf_matrix = table(prediction = forw_aic_prediction, actual = bbproj_tst$DivWin))
(forw_aic_sensitivity = forw_aic_conf_matrix[2, 2] / sum(forw_aic_conf_matrix[, 2]))
(forw_aic_specificity = forw_aic_conf_matrix[1, 1] / sum(forw_aic_conf_matrix[, 1]))
```

```{r}
opt_logistic_cutoff(forw_aic_model, cut_start = 0.1, cut_end = 0.8)
```

### Stepwise Search: AIC Model

```{r}
step_aic_model <- step(start_model, direction = "both", scope = scope, trace = 0)
summary(step_aic_model)
step_aic_prediction <- ifelse(predict(step_aic_model, bbproj_tst, type = "response") > 0.5, "Y", "N")
(step_aic_misclass = mean(step_aic_prediction != bbproj_tst$DivWin))
(step_aic_conf_matrix = table(prediction = step_aic_prediction, actual = bbproj_tst$DivWin))
(step_aic_sensitivity = step_aic_conf_matrix[2, 2] / sum(step_aic_conf_matrix[, 2]))
(step_aic_specificity = step_aic_conf_matrix[1, 1] / sum(step_aic_conf_matrix[, 1]))
```

```{r}
opt_logistic_cutoff(step_aic_model, cut_start = 0.1, cut_end = 0.8)
```