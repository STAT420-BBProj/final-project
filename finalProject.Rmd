---
title: "Final Project"
author: "Team 27 - Mark Berman (markcb2), Joel Kopp (joelk2), and Richard Wheeler (rw6)"
date: "7/23/2018"
output: 
  html_document:
     toc: yes
     toc_depth: 5
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width = 132)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
### function that calculates average percent error using actual wins and ###
### predicted wins from test data ###
calc_average_pct_error <- function(model) {
  test_predicted_wins <- predict(model, newdata = bbproj_tst)
  abs_dif <- abs(test_predicted_wins - bbproj_tst$W)
  avg_pct_error <- mean(abs_dif/test_predicted_wins) * 100
  rmse <- sqrt(mean(bbproj_tst$W - test_predicted_wins) ^ 2)
  list(predicted_wins = test_predicted_wins, 
       avg_pct_error = avg_pct_error, 
       rmse = rmse)
}
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
### function that plots actual wins versus predicted wins from test data
plot_predicted_wins_versus_actual_wins <- function(model_name, 
                                                   predicted_wins, 
                                                   actual_wins, 
                                                   avg_pct_error, rmse) {
  xrange <- c(0, max(actual_wins + 10))
  yrange <- c(0, max(predicted_wins + 10))
  title <- paste(model_name, " : 2014 - 2016 Baseball Seasons")
  sub <- paste("Average Percent Error: ", round(avg_pct_error,2), " %", 
               "   Test RMSE: ", round(rmse,2))
  plot(predicted_wins ~ actual_wins, 
       pch = 20, 
       col = "grey", 
       cex = 1.5, 
       xlab = "Actual Wins", 
       ylab = "Predicted Wins", 
       main= title, 
       sub=sub, 
       xlim=xrange, 
       ylim=yrange)
  abline(a = 0, b = 1, col = "darkorange")
}
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
### function that plots fitted values versus residuals for training data ###
plot_fitted_versus_residuals <- function(fitted_values, residuals, model_name) {
  plot(fitted_values, residuals, 
       col = "grey", 
       pch = 20, 
       xlab = "Fitted", 
       ylab = "Residuals", 
       main = model_name)
  abline(h = 0, col = "darkorange", lwd = 2)
}
```

```{r message = FALSE, warning = FALSE}
library(readr)
library(lmtest)
library(faraway)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### calculate cross validated RMSE for models using training data
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### calculate cross validated RMSE for models using training data
model_diagnostics = function(model) {
  print("Variance Inflation Factors")
  print(vif(model))
  print(paste("Number of coefficients with VIF > 5: ", sum(vif(model) > 5)))
  print(bptest(model))
  print(shapiro.test(resid(model)))
  print(paste("Leave One Out Cross-Validated RMSE: ", 
              round(calc_loocv_rmse(model), 2)))
  std_resid <- rstandard(model)[abs(rstandard(model)) > 2]
  is_std_resid_gt_five_percent <- length(std_resid) / n > 0.05
  print(ifelse(is_std_resid_gt_five_percent, "Outliers Exceed 5% of Obs", 
               "Outliers Do Not Exceed 5% of Obs"))
}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### load training data and clean it up ###
bbproj_trn <- read_csv("bbproj_trn.csv",na = c("", "NA"))
bbproj_trn$X1 <- NULL
bbproj_trn$lgID <- NULL
bbproj_trn$teamID <- NULL
bbproj_trn$divID <- NULL
bbproj_trn$Rank <- NULL
bbproj_trn$G <- NULL
bbproj_trn$Ghome <- NULL
bbproj_trn$L <- NULL
bbproj_trn$name <- NULL
bbproj_trn$teamIDBR   <- NULL
bbproj_trn$teamIDlahman45 <- NULL
bbproj_trn$teamIDretro <- NULL
bbproj_trn$DivWin <- as.factor(bbproj_trn$DivWin)
bbproj_trn$WCWin <- as.factor(bbproj_trn$WCWin)
bbproj_trn$LgWin <- as.factor(bbproj_trn$LgWin)
bbproj_trn$WSWin <- as.factor(bbproj_trn$WSWin)
bbproj_trn$franchID <- as.factor(bbproj_trn$franchID)
bbproj_trn$park <- as.factor(bbproj_trn$park)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
### load test data and clean it up ###
bbproj_tst <- read_csv("bbproj_tst.csv",na = c("", "NA"))
bbproj_tst$X1 <- NULL
bbproj_tst$lgID <- NULL
bbproj_tst$teamID <- NULL
bbproj_tst$divID <- NULL
bbproj_tst$Rank <- NULL
bbproj_tst$G <- NULL
bbproj_tst$Ghome <- NULL
bbproj_tst$L <- NULL
bbproj_tst$name <- NULL
bbproj_tst$teamIDBR   <- NULL
bbproj_tst$teamIDlahman45 <- NULL
bbproj_tst$teamIDretro <- NULL
bbproj_tst$DivWin <- as.factor(bbproj_tst$DivWin)
bbproj_tst$WCWin <- as.factor(bbproj_tst$WCWin)
bbproj_tst$LgWin <- as.factor(bbproj_tst$LgWin)
bbproj_tst$WSWin <- as.factor(bbproj_tst$WSWin)
bbproj_tst$franchID <- as.factor(bbproj_tst$franchID)
bbproj_tst$park <- as.factor(bbproj_tst$park)
```

***

# Introduction #

Conventional wisdom is that money buys happiness (winning) in Major League Baseball.  However, the advent of "Moneyball" in the early 2000s by the Oakland Athletics, Cleveland Indians, and other teams has lead to a more analytical approach to determining the make-up of Major League rosters.  

As it turns out, money is not the magic elixir when it comes to assembling a winning Major League Baseball (MLB) team.  The following plot shows that salary does not highly correlate with a winning record.  This is substantiated by the companion single linear regression model and summary statistics that show that salary, while significant, only has a marginal impact on wins by an MLB team. The adjusted *$R^2$* from the simple linear regression -- using training data from 2000 through 2013 -- is low.  Furthermore, the *average percent error* that compares actual wins versus predicted wins from the the test data (2014 through 2016) is high.

```{r echo=FALSE, message=FALSE, warning=FALSE}
options(scipen = 10)
xrange <- c(min(bbproj_trn$salary - 1000000), max(bbproj_trn$salary + 1000000))
yrange <- c(0, max(bbproj_trn$W + 10))
plot(bbproj_trn$W ~ bbproj_trn$salary, 
     pch = 20, 
     col = "grey", 
     cex = 1.5, 
     xlab = "Salary (US $)", 
     ylab = "Wins", 
     main= "Actual Salary vs Actual Wins (2000 - 2013)", 
     xlim=xrange, 
     ylim=yrange, 
     type="p")
abline(a = 81, b = 0, col = "darkorange")
legend("bottomright", 
        cex = 1.0, 
        bty = "n", 
        legend = c("training observations", "81 WINS"), 
        text.col = c("grey", "darkorange"),
        col = c("grey", "darkorange"), 
        pch = c(16, 15))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
salary_only_model <- lm(W ~ salary, data = bbproj_trn)
summary(salary_only_model)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", 
            round(calc_loocv_rmse(salary_only_model), 2)))
```

```{r message=FALSE, message=FALSE, warning=FALSE, include=FALSE}
### average percent error for the salary only model ###
result <- calc_average_pct_error(salary_only_model)
avg_pct_error_salary_only_model <- result$avg_pct_error
predicted_wins_salary_only_model <- result$predicted_wins
rmse_salary_only_model <- result$rmse
```

```{r echo=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search aic model
plot_predicted_wins_versus_actual_wins("Salary Only Model", 
                                       predicted_wins_salary_only_model, 
                                       bbproj_tst$W, 
                                       avg_pct_error_salary_only_model, 
                                       rmse_salary_only_model)
```

So what are the factors that move the needle?

We explore two related threads in attempting to identify the factors that have a strong impact on a winning baseball team.

The first thread uses multiple linear regression to identify the factors that influence a team's winning record over the course of a regular Major League Baseball (MLB) season.  
The second thread uses logistic regression to identify the factors that influence a team's ability to win their division.

## Data ##

A few words are in order about where we obtained the data from to perform this analysis.

The source of data is the Sean Lahman baseball archive (http://www.seanlahman.com/baseball-archive/), recognized by the *Society for American Baseball Research* (SABR) as the leading detailed player and team data archive from 1874 through the end of the 2017 Major League Baseball season.  We use team statistics from the years 2000 through 2013 as the training dataset and use data from 2014 through 2016 as the test dataset.  (As a note, we were unable to include the 2017 season in our analysis due to unavailability of salary data.)

### Variables ###

The team-based variables we examine fall into one of three categories: offensive, defensive, or administrative.

#### Administrative ####

- `yearID` - Year
- `franchID` - Franchise, or team name
- `W` - **Wins (the multiple linear regression response variable)**
- `DivWin` - **Division winner (Y or N factor- Logistic linear regression response variable)**
- `WCWin` - Wild Card winner (Y or N factor)
- `LgWin` - League champion (Y or N factor)
- `WSWin` - World Series winner (Y or N factor)
- `salary` - Team Salary (U.S dollars not adjusted for inflation)

#### Offensive ####

- `R` - Runs scored
- `AB` - At bats
- `H` - Total hits (including doubles, triples, and home runs)
- `X1B` - Singles
- `X2B` - Doubles
- `X3B` - Triples
- `HR` - Home runs
- `BB` - Base on balls (walks)
- `SO` - Strikeouts
- `SB` - Stolen bases
- `CS` - Caught stealing
- `HBP` - Hit by pitch
- `SF` - Sacrifice flies
- `park` - Name of team's home ballpark (factor)
- `attendance` - Home attendance total
- `BPF` - Three-year park factor for batters
- `GIDP` - Grounded into double plays
- `RBI` - Runs Batted In
- `IBB` - Intentional walks
- `TB` - Total bases
- `SLG` - Slugging percentage
- `OBP` - On-base percentage
- `OPS` - On-base plus slugging percentage
- `BABIP` - Batting average for balls in play
- `RC` - Runs created
- `uBB` - Unintentional walks
- `wOBA` - Weighted on-base average

#### Defensive ####

- `RA` - Total runs allowed
- `ER` - Earned runs allowed
- `ERA` - Earned run average
- `CG` - Complete games
- `SHO` - Shutouts
- `SV` - Saves
- `IPOuts` - Outs pitched
- `HA` - Hits allowed
- `HRA` - Home runs allowed
- `BBA` - Walks allowed
- `SOA` - Strikeouts by pitchers
- `E` - Errors
- `DP` - Double plays
- `FP` - Fielding percentage
- `PPF` - Three-year park factor for pitchers
- `WHIP` - Walks and hits per innings pitched

Notes regarding data preparation may be found in Appendix A, and the calculations end explanations of some of the advanced Sabermetrics included in our dataset may be found in Appendix B.

***

# The Search for Better Multiple Linear Regression Models For Predicting Regular Season Team Wins #

## Methodology ##

The goal of this thread is to find a Multiple Linear Regression (MLR) model that is simple enough to explain the relationship between the response variable (regular season wins) and the predictors.  In essence, we are interested in a reasonably small MLR model that is easy to interpret.  Equally important is the need for the model to predict well against the test data set from the 2014, 2015 and 2016 MLB regular seasons.  This means we have to sacrifice a certain degree of model simplicity.  In short, we are trying to find a model that will allow us to "have our cake and eat it too".

The approach we take is to employ all three search procedures -- *Backward*, *Forward* and *Stepwise* -- against the *Akaike Information Criterion* (AIC) and the *Bayesian Information Criterion* (BIC) quality criterion in order to find the model that best meets the goals of this thread.

We start by creating the following six initial models. 

- Backward Search - BIC Model
- Backward Search - AIC Model
- Step Search - BIC Model
- Step Search - AIC Model
- Forward Search - BIC Model
- Forward Search - AIC Model.

We then use an iterative process of model evaluation and refinement until we have produced a lineup of candidate models that move on to the next phase of assessment:  prediction against the test dataset.

The model evaluation and refinement process utilizes the following six diagnostics tests.

- Breusch-Pagan Test (BP Test) to assess whether each of the six initial models meets the equal variance of residuals requirement.  A *p_value* greater than *0.05* indicates a high likelihood that a given model's errors have constant variance.
- Shapiro-Wilk Test to assess whether each of the six initial models meet the normal distribution of residuals requirement.  A *p_value* greater than *0.05* indicates a high likelihood that a given model's errors are normally distributed.
- A count of the number of unusual observations.  The test counts the number of standardized residuals that exceed a magnitude of 2. The threshold for concern is when the count exceeds 5% of the total number of observations.
- Leave One Out Cross-Validated RMSE (LOOCV-RMSE) to assess how well a given model generalizes to unseen observations (e.g., the test data).  The smaller the value, the more likely a given model
will do a good job predicting the response variable (in this case, regular season wins by an MLB team).
- Calculation of the Variance Inflation Factor (VIF) for each of the estimated predictor coefficients ($\hat{\beta}_j$).  A VIF score greater than 5 indicates that a given predictor is highly collinear.
- A check to make sure that the each estimated predictor coefficient ($\hat{\beta}_j$) is likely significant by rejecting the null hypothesis $H_0: B_j = 0$.  We fail to reject $H_0: B_j = 0$ when $\hat{\beta}_j$ has a *p_value* greater than *$\alpha = 0.05$*.

If a model passes all six of these diagnostic tests, it is considered worthy of being evaluated against the test dataset.  From a nomenclature perspective, each model that passes this hurdle is called a *candidate* model.  If a model does not pass all six of these diagnostic tests, we refit the model after removing a single predictor.  We then perform the same six diagnostic tests. We repeat this cycle until a model is produced that passes all six of the diagnostic tests. 

Once *candidate* models are identified, we assess their ability to predict regular season team wins against the test dataset.  The *candidate* model with the lowest 
*Average Percent Error* and the lowest *Test RMSE* is declared the winner.

Please note that the response variable is designated as **`W`** in models that are described below.

### The Six Initial Multiple Linear Regression Models ###

```{r}
scope <- W ~ R + AB + H + X2B + X3B + HR + BB + SO + SB + CS + HBP + SF + RA + 
  ER + ERA + CG + SHO + SV + IPouts + HA  + HRA + BBA + SOA + E + DP + FP + 
  attendance + BPF + PPF + salary + RBI + GIDP + IBB + TB + SLG + OBP + OPS + 
  WHIP + BABIP + RC + X1B + uBB + wOBA

formula <- formula(scope)
start_model <- lm(formula, data= bbproj_trn)
n <- length(resid(start_model))
step_search_start_model <- lm(W ~ 1, data= bbproj_trn)
```

#### Backwards Search: BIC Model ####

```{r}
### Backwards Search: BIC Model
bic_model <- step(start_model,  direction = "backward", k = log(n), trace = 0)
summary(bic_model)
```

#### Backwards Search: AIC Model ####

```{r}
### Backwards Search: AIC Model
aic_model <- step(start_model, direction = "backward", trace = 0)
summary(aic_model)

```

#### Step Search: BIC Model ####

```{r}
### Step Search: BIC Model
step_bic_model <- step(step_search_start_model, scope = scope, direction = "both",
                       k = log(n), trace = 0)
summary(step_bic_model)
```


#### Step Search: AIC Model ####

```{r}
### Step Search: AIC Model ### 
step_aic_model <- step(step_search_start_model, scope = scope, 
                       direction = "both", trace = 0)
summary(step_aic_model)
```


#### Forward Search: BIC Model ####

```{r}
bic_forward_model <- step(step_search_start_model, scope = scope, 
                          k = log(n), trace = 0)
summary(bic_forward_model)
```

#### Forward Search: AIC Model ####

```{r}
aic_forward_model <- step(step_search_start_model, scope = scope, trace = 0)
summary(aic_forward_model)
```

### Identifying which of the Six Initial Models make the Cut as Candidate Models ###

#### Evaluation of the Backwards Search: BIC Model ####
```{r}
### Breusch-Pagan Test on Backwards Search - BIC Model
bptest(bic_model)
```

```{r}
plot_fitted_versus_residuals(fitted(bic_model), 
                             resid(bic_model), 
                             "Backwards Search - BIC Model")
```

```{r}
### Shapiro - Wilk Normality Test on Backwards Search - BIC Model ###
shapiro.test(resid(bic_model))
```

```{r}
qqnorm(resid(bic_model), 
       main = "Normal Q-Q Plot, Backwards Search - BIC Model", 
       col = "darkgrey")
qqline(resid(bic_model), col = "dodgerblue", lwd = 2)
```

```{r}
print(paste("Leave One Out Cross-Validated RMSE: ", 
            round(calc_loocv_rmse(bic_model), 2)))
```

```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total 
### observations -- Backwards Search: BIC Model
std_resid_bic_model <- rstandard(bic_model)[abs(rstandard(bic_model)) > 2]
is_std_resid_gt_five_percent_bic_model <- 
  length(std_resid_bic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_bic_model, 
       "Outliers Exceed 5% of Obs", 
       "Outliers Do Not Exceed 5% of Obs")
```

```{r}
### VIF > 5 for Backwards Search: BIC Model Coefficients
vif_bic_model <- vif(bic_model)
vif_bic_model[which(vif_bic_model > 5)]
```

#### Refining the Backwards Search: BIC Model Due to High Variance Inflation Factors ####

```{r message = FALSE, warning = FALSE}
library(caret)
```

```{r}
bic_model_high_vif_cols <- c("R", "AB", "H", "BB", "SO", "RA", "E", "FP",
                             "BABIP", "RC")
indices_to_drop <- findCorrelation(cor(bbproj_trn[,c(bic_model_high_vif_cols)]), 
                                   cutoff = 0.6)
(vars_to_drop <- bic_model_high_vif_cols[indices_to_drop])
```

##### A Better and Smaller Backwards Search: BIC Model #####

```{r}
smaller_bic_model <- lm(W ~ AB + BB + SO + SF + RA + CG + SV + IPouts + BBA + 
                          BABIP, data = bbproj_trn)
summary(smaller_bic_model)
```

##### Diagnostics for the Better and Smaller Backwards Search: BIC Model #####

```{r}
model_diagnostics(smaller_bic_model)
```

The Smaller Backward Search: BIC model passes all six of the diagnostic tests.  It is deemed as a candidate model for the next phase: evaluation of the model's predictive capability against the test dataset.

#### Evaluation of the Backwards Search: AIC Model ####

```{r}
### Breusch-Pagan Test on AIC Model
bptest(aic_model)
```

```{r}
plot_fitted_versus_residuals(fitted(aic_model), resid(aic_model), 
                             "Backwards Search - AIC Model")
```

```{r}
### Shapiro - Wilk Normality Test on Backwards Search - AIC Model ###
shapiro.test(resid(aic_model))
```

```{r}
qqnorm(resid(aic_model), 
       main = "Normal Q-Q Plot, Backwards Search - AIC Model", 
       col = "darkgrey")
qqline(resid(aic_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", 
            round(calc_loocv_rmse(aic_model), 2)))
```

```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total 
### observations -- Backwards Search: AIC Model
std_resid_aic_model <- rstandard(aic_model)[abs(rstandard(aic_model)) > 2]
is_std_resid_gt_five_percent_aic_model <- 
  length(std_resid_aic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent_aic_model, 
       "Outliers Exceed 5% of Obs", 
       "Outliers Do Not Exceed 5% of Obs")
```


```{r}
### VIF > 5 for Backwards Search: AIC Model Coefficients
vif_aic_model <- vif(aic_model)
vif_aic_model[which(vif_aic_model > 5)]
```

#### Refining the Backwards Search: AIC Model Due to High Variance Inflation Factors ####

```{r}
aic_model_high_vif_cols <- c("R", "AB", "H", "X2B", "X3B", "HR", "BB", "SO", 
                             "SF", "RA", "ER", "IPouts", "E", "FP","BPF", 
                             "PPF", "IBB", "OBP", "BABIP", "RC", "wOBA")
indices_to_drop <- findCorrelation(cor(bbproj_trn[,c(aic_model_high_vif_cols)]), 
                                   cutoff = 0.6)
(vars_to_drop <- aic_model_high_vif_cols[indices_to_drop])
```

##### A Better and Smaller Backwards Search: AIC Model #####

```{r}
smaller_aic_model <-  lm(formula = W ~  HR + BB + SO + ER + CG + SHO + SV + 
                           IPouts + BBA + FP + GIDP + BABIP, data = bbproj_trn)
summary(smaller_aic_model)
```

##### Diagnostics for the Better and Smaller Backwards Search: AIC Model #####

```{r}
model_diagnostics(smaller_aic_model)
```

The Smaller Backward Search: AIC model does not pass all six of the diagnostic tests.  The Shapiro-Wilk p-value is smaller than the threshold of $\alpha = 0.05$. As a result, we reject the null hypothesis that the residuals are normally distributed. As such, this model is not deemed a *candidate* model and is not passed on to the next evaluation phase.

#### Evaluation of the Step Search: BIC Model ####

```{r}
### Breusch-Pagan Test on Step BIC Model
bptest(step_bic_model)
```

```{r}
plot_fitted_versus_residuals(fitted(step_bic_model), resid(step_bic_model), 
                             "Step Search - BIC Model")
```

```{r}
### Shapiro - Wilk Normality Test on Step Search - BIC Model ###
shapiro.test(resid(step_bic_model))
```

```{r}
qqnorm(resid(step_bic_model), 
       main = "Normal Q-Q Plot, Step Search - BIC Model", 
       col = "darkgrey")
qqline(resid(step_bic_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", 
            round(calc_loocv_rmse(step_bic_model), 2)))
```

```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total
### observations -- Step Search: BIC Model
std_resid_step_bic_model <- rstandard(
  step_bic_model)[abs(rstandard(step_bic_model)) > 2]
is_std_resid_gt_five_percent__step_bic_model <- length(
  std_resid_step_bic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent__step_bic_model, "Exceeds 5% of Obs", 
       "Does Not Exceed 5% of Obs")
```


```{r}
### VIF > 5 for Step Search: BIC Model Coefficients
vif_step_bic_model <- vif(step_bic_model)
vif_step_bic_model[which(vif_step_bic_model > 5)]
```

#### Refining the Step Search: BIC Model Due to High Variance Inflation Factors ####

##### A Better and Smaller Step Search: BIC Model #####

```{r}
smaller_step_bic_model <- lm(formula = W ~ SV + R + RA + SHO + CG + X3B + 
                               IPouts + AB + salary, data = bbproj_trn)
summary(smaller_step_bic_model)
```

##### Diagnostics for the Better and Smaller Step Search: BIC Model #####

```{r}
model_diagnostics(smaller_step_bic_model)
```

The Smaller Step Search: BIC model passes all six of the diagnostic tests.  It is deemed as a *candidate* model for the next phase: evaluation of the model's predictive capability against the test dataset.

#### Evaluation of the Step Search: AIC Model ####

```{r}
### Breusch-Pagan Test on Step AIC Model
bptest(step_aic_model)
```

```{r}
plot_fitted_versus_residuals(fitted(step_aic_model), resid(step_aic_model), 
                             "Step Search - AIC Model")
```

```{r}
### Shapiro - Wilk Normality Test on Step Search - AIC Model ###
shapiro.test(resid(step_aic_model))
```

```{r}
qqnorm(resid(step_aic_model), 
       main = "Normal Q-Q Plot, Step Search - AIC Model", 
       col = "darkgrey")
qqline(resid(step_aic_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", 
            round(calc_loocv_rmse(step_aic_model), 2)))
```

```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total
### observations -- Step Search: AIC Model
std_resid_step_aic_model <- rstandard(
  step_aic_model)[abs(rstandard(step_aic_model)) > 2]
is_std_resid_gt_five_percent__step_aic_model <- length(
  std_resid_step_aic_model) / n > 0.05
ifelse(is_std_resid_gt_five_percent__step_aic_model,
       "Exceeds 5% of Obs", 
       "Does Not Exceed 5% of Obs")
```

```{r}
### VIF > 5 for Step Search: AIC Model Coefficients
vif_step_aic_model <- vif(step_aic_model)
vif_step_aic_model[which(vif_step_aic_model > 5)]
```

#### Refining the Step Search: AIC Model Due to High Variance Inflation Factors ####

##### A Better and Smaller Step Search: AIC Model #####

```{r}
smaller_step_aic_model <- lm(formula = W ~ SV + R + SHO + CG + X3B + IPouts + 
                               BBA + HRA + SOA, data = bbproj_trn)
summary(smaller_step_aic_model)
```

##### Diagnostics for the Better and Smaller Step Search: AIC Model #####

```{r}
model_diagnostics(smaller_step_aic_model)
```

The Smaller Step Search: AIC model passes all six of the diagnostic tests.  It is deemed as a *candidate* model for the next phase: evaluation of the model's predictive capability against the test dataset.

#### Evaluation of the Forward Search: BIC Model ####
```{r}
### Breusch-Pagan Test
bptest(bic_forward_model)
```

```{r}
plot_fitted_versus_residuals(fitted(bic_forward_model), 
                             resid(bic_forward_model), 
                             "Forward Search - BIC Model")
```

```{r}
### Shapiro - Wilk Normality Test ###
shapiro.test(resid(bic_forward_model))
```

```{r}
qqnorm(resid(bic_forward_model), 
       main = "Normal Q-Q Plot, Step Search - BIC Model", col = "darkgrey")
qqline(resid(bic_forward_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", 
            round(calc_loocv_rmse(bic_forward_model), 2)))
```

```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total 
### observations -- Step Search: AIC Model
std_resid <- rstandard(bic_forward_model)[abs(rstandard(bic_forward_model)) > 2]
is_std_resid_gt_five_percent <- length(std_resid) / n > 0.05
ifelse(is_std_resid_gt_five_percent, "Exceeds 5% of Obs", 
       "Does Not Exceed 5% of Obs")
```

```{r}
### VIF > 5 for Step Search: AIC Model Coefficients
vifs <- vif(bic_forward_model)
vifs[which(vifs > 5)]
```

#### Refining the Forward Search: BIC Model Due to High Variance Inflation Factors ####

```{r}
high_vif_cols <- as.array(names(vifs[which(vifs > 5)]))
indices_to_drop <- findCorrelation(cor(bbproj_trn[,c(high_vif_cols)]), 
                                   cutoff = 0.6)
(vars_to_drop <- high_vif_cols[indices_to_drop])
```

##### A Better and Smaller Forward Search: BIC Model #####

```{r}
terms = attr(bic_forward_model$terms, "term.labels")
terms = terms[! terms %in% vars_to_drop]
formula = as.formula(paste("W ~", paste(terms, collapse = "+")))
smaller_model <- lm(formula = formula, data = bbproj_trn)
summary(smaller_model)
smaller_bic_forward_model = smaller_model
```

##### Diagnostics for the Better and Smaller Forward Search: BIC Model #####

```{r}
model_diagnostics(smaller_bic_forward_model)
```

The Smaller Forward Search: BIC model passes all six of the diagnostic tests.  It is deemed as a *candidate* model for the next phase: evaluation of the model's predictive capability against the test dataset.

#### Evaluation of the Forward Search: AIC Model ####

```{r}
### Breusch-Pagan Test
bptest(aic_forward_model)
```

```{r}
plot_fitted_versus_residuals(fitted(aic_forward_model), 
                             resid(aic_forward_model), 
                             "Forward Search - AIC Model")
```

```{r}
### Shapiro - Wilk Normality Test ###
shapiro.test(resid(aic_forward_model))
```

```{r}
qqnorm(resid(aic_forward_model), 
       main = "Normal Q-Q Plot, Step Search - AIC Model", col = "darkgrey")
qqline(resid(aic_forward_model), col = "dodgerblue", lwd = 2)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
print(paste("Leave One Out Cross-Validated RMSE: ", 
            round(calc_loocv_rmse(aic_forward_model), 2)))
```

```{r}
### Do the number of standard residuals greater than 2 exceed 5% of the total 
### observations -- Step Search: AIC Model
std_resid <- rstandard(aic_forward_model)[abs(rstandard(aic_forward_model)) > 2]
is_std_resid_gt_five_percent <- length(std_resid) / n > 0.05
ifelse(is_std_resid_gt_five_percent,"Exceeds 5% of Obs", 
       "Does Not Exceed 5% of Obs")
```

```{r}
### VIF > 5 for Step Search: AIC Model Coefficients
vifs <- vif(aic_forward_model)
vifs[which(vifs > 5)]
```

#### Refining the Forward Search: AIC Model Due to High Variance Inflation Factors ####

```{r}
high_vif_cols <- as.array(names(vifs[which(vifs > 5)]))
indices_to_drop <- findCorrelation(cor(bbproj_trn[,c(high_vif_cols)]), 
                                   cutoff = 0.6)
vars_to_drop <- high_vif_cols[indices_to_drop]
vars_to_drop
```

##### A Better and Smaller Forward Search: AIC Model #####

```{r}
terms = attr(aic_forward_model$terms, "term.labels")
terms = terms[! terms %in% vars_to_drop]
formula = as.formula(paste("W ~", paste(terms, collapse = "+")))
smaller_model <-  lm(formula = formula, data = bbproj_trn)
summary(smaller_model)
smaller_aic_forward_model = smaller_model
```

##### Diagnostics for the Better and Smaller Forward Search: AIC Model #####

```{r}
model_diagnostics(smaller_aic_forward_model)
```

The Smaller Step Search: AIC model passes all six of the diagnostic tests.  It is deemed as a *candidate* model for the next phase: evaluation of the model's predictive capability against the test dataset. 

### Validate the Candidate Model Predictive Effectiveness using Test Data  ###

We now move forward with five *candidate* models and predict MLB regular season team wins with our test dataset from the 2014, 2015 and 2016 MLB regular seasons.  (There are a total of 90 observations in the test dataset -- one observation for each team for 2014, 2015 and 2016.)

```{r echo=FALSE, message=FALSE, warning=FALSE}
### average percent error for the smaller backwards search bic model ###
result <- calc_average_pct_error(smaller_bic_model)
avg_pct_error_smaller_bic_model <- result$avg_pct_error
predicted_wins_smaller_bic_model <- result$predicted_wins
rmse_wins_smaller_bic_model <- result$rmse
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search bic model
plot_predicted_wins_versus_actual_wins("Smaller Backwards Search BIC Model",
                                       predicted_wins_smaller_bic_model, 
                                       bbproj_tst$W, 
                                       avg_pct_error_smaller_bic_model, 
                                       rmse_wins_smaller_bic_model)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### average percent error for the smaller step search bic model ###
result <- calc_average_pct_error(smaller_step_bic_model)
avg_pct_error_smaller_step_bic_model <- result$avg_pct_error
predicted_wins_smaller_step_bic_model <- result$predicted_wins
rmse_wins_smaller_step_bic_model <- result$rmse
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search bic model
plot_predicted_wins_versus_actual_wins("Smaller Step Search BIC Model", 
                                       predicted_wins_smaller_step_bic_model, 
                                       bbproj_tst$W, 
                                       avg_pct_error_smaller_step_bic_model, 
                                       rmse_wins_smaller_step_bic_model)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
### average percent error for the smaller step search aic model ###
result <- calc_average_pct_error(smaller_step_aic_model)
avg_pct_error_smaller_step_aic_model <- result$avg_pct_error
predicted_wins_smaller_step_aic_model <- result$predicted_wins
rmse_smaller_step_aic_model <- result$rmse
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search aic model
plot_predicted_wins_versus_actual_wins("Smaller Step Search AIC Model", 
                                       predicted_wins_smaller_step_aic_model, 
                                       bbproj_tst$W, 
                                       avg_pct_error_smaller_step_aic_model,
                                       rmse_smaller_step_aic_model)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### average percent error for the smaller backwards search bic model ###
result <- calc_average_pct_error(smaller_bic_forward_model)
avg_pct_error_smaller_bic_forward_model <- result$avg_pct_error
predicted_wins_smaller_bic_forward_model <- result$predicted_wins
rmse_wins_smaller_bic_forward_model <- result$rmse
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search bic model
plot_predicted_wins_versus_actual_wins("Smaller Forward Search BIC Model", predicted_wins_smaller_bic_forward_model, bbproj_tst$W, 
                                       avg_pct_error_smaller_bic_forward_model,
                                       rmse_wins_smaller_bic_forward_model)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### average percent error for the smaller backwards search bic model ###
result <- calc_average_pct_error(smaller_aic_forward_model)
avg_pct_error_smaller_aic_forward_model <- result$avg_pct_error
predicted_wins_smaller_aic_forward_model <- result$predicted_wins
rmse_wins_smaller_aic_forward_model <- result$rmse
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
### Plot predicted vs actual wins for the smaller step search bic model
plot_predicted_wins_versus_actual_wins("Smaller Forward Search AIC Model",
                                       predicted_wins_smaller_aic_forward_model,
                                       bbproj_tst$W, 
                                       avg_pct_error_smaller_aic_forward_model, 
                                       rmse_wins_smaller_aic_forward_model)
```

## Results ## 

### And the Winning Linear Regression Model is... ###

While all models do a good job predicting the number of regular season wins by an MLB team, the **Smaller Step Search: BIC Model** produces the best results against the test dataset.  This model has the lowest *Average Percent Error* score and the lowest *Test RMSE* score for predictions with the test dataset.  This model also generalizes the best predictions against unseen observations.  This means there is a high likelihood this model will perform well against test data for the 2017 and 2018 MLB seasons.  The model also contains the second smallest number of predictors and also contains *Salary* as one of the predictors.  This means it is easy to explain the relationship between the response variable **`W`** and it predictors.  The fact that *Salary* is included as a predictor supports the original premise that *Salary*, while not a dominant predictor, is a marginally significant predictor of regular season MLB team wins.

The following table summarizes the key evaluation factors used to choose the best model from the five candidates.

```{r, message=FALSE, warning=FALSE, include=FALSE}
number_of_predictors <- c(10, 9, 9, 6,15)
salary_included_as_predictor <- c(FALSE, TRUE, FALSE, FALSE, TRUE)
loocv_RMSE <- c(4.95, 2.99, 4.05, 4.2,6.27)
average_percent_error <- c(5.51, 3.17, 4.07, 5.46, 6.27)
test_rmse <- c(0.47, 0.32, 0.97, 1.55, 3.25)
results_df <- data.frame(predictor_count = number_of_predictors, 
                         salary_included = salary_included_as_predictor, 
                         loocv_RMSE = loocv_RMSE, 
                         avg_pct_error= average_percent_error, 
                         test_rmse = test_rmse )
row.names(results_df) <- c("Smaller Backward Search: BIC Model", 
                           "Smaller Step Search: BIC Model", 
                           "Smaller Step Search: AIC Model",
                           "Smaller Forward Search: BIC Model", 
                           "Smaller Forward Search AIC Model")
colnames(results_df) <- c("Predictor Count", "Salary Included", 
                          "LOOCV_RMSE", 
                          "AVG % Error", 
                          "Test RMSE")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
kable(results_df, format = "markdown", padding = 4, align=c(rep('c', times=5)))
```

## Discussion ##

The winning model -- *Smaller Step Search: BIC* -- adds validity to the axiom that pitching wins games.  The key predictors that influence the number of regular season wins by an MLB team are related to pitching.

- Holding all other predictors constant, each additional *Save* increases the average number of wins by 0.40.
- Holding all other predictors constant, each additional *Shutout* increases the average number of wins by 0.17.
- Holding all other predictors constant, each additional *Complete Game* increases the average number of wins by 0.142.
- Holding all other predictors constant, each additional run given up by a pitcher (*Runs Against*) reduces the average number of wins by 0.7.

It is moderately surprising that offense-related predictors do not dominate the model. (One of the authors of this study is a die-hard New York Yankees fan.  The New York Yankees are well known for slightly above average pitching but dominant offense.  The Yankees have 27 World Championships to date.  That said, the last championship occurred nine years ago.  So maybe there is more to good pitching than meets the eye.)  The one offense-related predictor that moves the needle is *Runs*.  Holding all other predictors constant, an increase in 1 run per game increases the average number of wins by 0.10.

The most exciting play in baseball, the *Triple*, is negatively correlated with wins.  Holding all other predictors constant, an increase of 1 *Triple* reduces the average number of wins by 0.5.  This is a real "head-scratcher."

***

# Logistic Modeling and Prediction of Division Winners #

## Salary as a Predictor for Division Winners ##

For this second thread, we turn our attention to logistic regression and our ability to classify and predict division winners using our team predictor set.  First, we see how well *Salary* alone can predict pennants.

```{r}
salary_only_model <- glm(DivWin ~ salary, data = bbproj_trn, family = binomial)
(salary_only_summary = summary(salary_only_model))
plot(as.numeric(DivWin) - 1 ~ salary, data = bbproj_trn, 
     pch = 20, 
     main = "Probability of Buying a Division Winner",
     ylab = "Probability of Winning Division", 
     xlab = "Team Salary ($)",  
     xlim = c(0, 3e8))
curve(predict(salary_only_model, data.frame(salary = x), type = "response"), 
      add = TRUE, 
      col = "tomato", 
      lty = 2,
      lwd = 2)
```

Graphically, a team's payroll seems to be moderately influential in predicting their chances of taking home a division crown.  Indeed, our Wald test for *Salary* alone yields a p-value of `r salary_only_summary$coefficients["salary", "Pr(>|z|)"]`, allowing us to reject the null hypothesis ($H_0 : \beta_{salary} = 0$) for any reasonable value for $\alpha$.  So what happens when we look at our salary model's test misclassification rate?

```{r}
salary_prediction <- ifelse(predict(salary_only_model,
                                    bbproj_tst,
                                    type = "response") > 0.5, "Y", "N")
(prevalence = table(bbproj_tst$DivWin) / nrow(bbproj_tst))
(salary_misclass = mean(salary_prediction != bbproj_tst$DivWin))
```

First, we examine prevalence of division winners.  We see that `r prevalence[2] * 100`% of the teams were division winners, which makes sense since there are 6 divisions and 30 MLB teams; therefore, you will only have 6 division winners per year.  Our salary model has a misclassification rate of `r round(salary_misclass, 3)`, which is worse than our prevalence. We would have a better misclassification rate if we simply stated that there are no division winners! We can certainly do better than this.

## Methodology ##

Similar to the MLR section, the ultimate goal of this thread is to find a logistic regression model that predicts well and is easily interpretable.  To facilitate interpretability, we utilize the **`step`** function to capture significant predictors using both AIC and BIC criteria, searching "forward", "backward", and "stepwise".  We strive to minimize the number of predictors while maximizing the model's predictive capabilities.

To predict efficiently, we develop Bayes classifiers based on the models and evaluate them by calculating their sensitivity, specificity, and misclassification rate using our test dataset from the 2014, 2015, and 2016 MLB seasons.  Finally, we cross validate the misclassifications and choose a model that has the lowest 5-fold cross validation misclassification rate while keeping the other diagnostics in mind.

### Candidate Logistic Regression Models ###

We will begin our search for a better classifier by setting up an initial logistic regression model contain all of the predictors we would like to evaluate.  Then, we will proceed to eliminate predictors using backwards, forwards, and stepwise AIC and BIC searches.  See Appendix C for a complete evaluation of the remaining model derivations and classifications.  The following will detail the methodology using our best candidate model.

First, we set our scope and generate our initial Generalized Linear Model with only the intercept as the predictor.

```{r warning=FALSE}
scope <- DivWin ~ W + R + AB + H + X2B + X3B + HR + BB + SO + SB + CS + HBP + 
  SF + RA + ER + ERA + CG + SHO + SV + IPouts + HA  + HRA + BBA + SOA + E + 
  DP + FP + attendance + BPF + PPF + salary + RBI + GIDP + IBB + TB + SLG + 
  OBP + OPS + WHIP + BABIP + RC + X1B + uBB + wOBA
start_model <- glm(DivWin ~ 1, data = bbproj_trn, family = binomial)
n <- length(resid(start_model))
```

#### Forward Search: AIC Model ####

Next, we begin to determine which predictors are influential by using the **`step`** function.  In this case, we are using a forward-search algorithm using AIC criteria.

```{r warning=FALSE}
aic_model <- step(start_model, direction = "forward", scope = scope, 
                  trace = 0)
summary(aic_model)
```

Our forward-search using AIC yields the following logistic model

\[
\log \bigg(\frac{P[DivWin = 1]}{1 - P[DivWin = 1]} \bigg) = \beta_0 + \beta_W W + \beta_{X2B} X2B + \beta_{HA} HA + \beta_{DP} DP + \beta_{GIDP} GIDP
\]

where the log odds of a team winning their division is dependent upon cumulative wins, doubles, hits against, turned double plays, and double plays grounded into.

##### Diagnostics #####

First, we evaluate our model with our training data by examining its 5-fold cross-validation misclassification rate.  We choose this diagnostic over the training misclassification rate to avoid giving preference to larger models.  This will have the added bonus of reducing the probability of overfitting.

Here we develop a simple function to perform our cross validation (see the **`cross_validate`** function in [Appendix D](#cross_validate)).

```{r cross_validate, echo = FALSE}
cross_validate = function(model, data = bbproj_trn, folds = 5) {
  set.seed(42)
  boot::cv.glm(data, model, K = folds)$delta[1]
}
```

```{r}
(aic_cv <- cross_validate(aic_model))
```

This specific model performed the best out of the six selected logistic regression models with a 5-fold cross-validation misclassification rate of `r round(aic_cv, 3)`.

Next, we will create a function that will evaluate this model using a confusion matrix of our test data to calculate its sensitivity, specificity, and its misclassification rate (the **`classify_and_diagnose`** function code can be found in [Appendix D](#classify_and_diagnose)).  We will proceed with a default cutoff value of 0.5.

```{r classify_and_diagnose, echo = FALSE}
classify_and_diagnose = function(model, data = bbproj_tst, 
                                 actual = bbproj_tst$DivWin, 
                                 pos = "Y", neg = "N", cutoff = 0.5) {
  # Generate a classifier given the model, data, cutoff, and positive and
  # negative responses
  pred = ifelse(predict(model, 
                        data, 
                        type = "response") > cutoff, pos, neg)
  
  # Generate the confusion matrix
  conf_mat = table(prediction = pred, actual = actual)

  # Calculate sensitivity, specificity, and the misclassification rate and
  # return them plus the confusion matrix
  list(confusion_matrix = conf_mat, 
       sensitivity = conf_mat[2, 2] / sum(conf_mat[, 2]), 
       specificity = conf_mat[1, 1] / sum(conf_mat[, 1]), 
       misclassification = mean(pred != actual))
}
```

```{r}
(aic_diag <- classify_and_diagnose(aic_model))
```

Here, we beat the salary-only model (along with the prevalence) with a test misclassification rate of `r round(aic_diag$misclassification, 3)`.  Our specificity looks great for this model at `r round(aic_diag$specificity, 3)`.  Our `r aic_diag$confusion_matrix[1, 2]` false negatives could come down by adjusting our cutoff value, but the trade-off would be a lower specificity and a worse misclassification rate (that exercise is out of scope for this project).

```{r glm_back_bic, warning = FALSE, echo=FALSE}
formula <- formula(scope)
back_bic_model <- step(glm(formula, data = bbproj_trn, family = binomial), 
                       direction = "backward", k = log(n), trace = 0)
back_bic_diag <- classify_and_diagnose(back_bic_model)
back_bic_cv <- cross_validate(back_bic_model)
```

```{r glm_forw_bic, echo = FALSE}
forw_bic_model <- step(start_model, direction = "forward", scope = scope, 
                       k = log(n), trace = 0)
forw_bic_diag <- classify_and_diagnose(forw_bic_model)
forw_bic_cv <- cross_validate(forw_bic_model)
```

```{r glm_step_bic, echo = FALSE}
step_bic_model <- step(start_model, direction = "both", scope = scope, 
                       k = log(n), trace = 0)
step_bic_diag <- classify_and_diagnose(step_bic_model)
step_bic_cv <- cross_validate(step_bic_model)
```

```{r glm_back_aic, warning = FALSE, echo = FALSE}
formula <- formula(scope)
back_aic_model <- step(glm(formula, data = bbproj_trn, family = binomial), 
                       direction = "backward", trace = 0)
back_aic_diag <- classify_and_diagnose(back_aic_model)
back_aic_cv <- cross_validate(back_aic_model)
```

```{r glm_forw_aic, echo = FALSE}
forw_aic_model <- step(start_model, direction = "forward", 
                       scope = scope, trace = 0)
forw_aic_diag <- classify_and_diagnose(forw_aic_model)
forw_aic_cv <- cross_validate(forw_aic_model)
```

```{r glm_step_aic, echo = FALSE}
step_aic_model <- step(start_model, direction = "both", 
                       scope = scope, trace = 0)
step_aic_diag <- classify_and_diagnose(step_aic_model)
step_aic_cv <- cross_validate(step_aic_model)
```

***

## Results ##

```{r include = FALSE}
logistic_model_results = data.frame(
  cv = c(forw_aic = forw_aic_cv,
         back_aic = back_aic_cv,
         step_aic = step_aic_cv,
         forw_bic = forw_bic_cv,
         back_bic = back_bic_cv,
         step_bic = step_bic_cv),
  nparams = c(forw_aic = length(coef(forw_aic_model)),
              back_aic = length(coef(back_aic_model)),
              step_aic = length(coef(step_aic_model)),
              forw_bic = length(coef(forw_bic_model)),
              back_bic = length(coef(back_bic_model)),
              step_bic = length(coef(step_bic_model))),
  sensitivity = c(forw_aic = forw_aic_diag$sensitivity,
                  back_aic = back_aic_diag$sensitivity,
                  step_aic = step_aic_diag$sensitivity,
                  forw_bic = forw_bic_diag$sensitivity,
                  back_bic = back_bic_diag$sensitivity,
                  step_bic = step_bic_diag$sensitivity),
  specificity = c(forw_aic = forw_aic_diag$specificity,
                  back_aic = back_aic_diag$specificity,
                  step_aic = step_aic_diag$specificity,
                  forw_bic = forw_bic_diag$specificity,
                  back_bic = back_bic_diag$specificity,
                  step_bic = step_bic_diag$specificity),
  fn = c(forw_aic = forw_aic_diag$confusion_matrix[1, 2],
         back_aic = back_aic_diag$confusion_matrix[1, 2],
         step_aic = step_aic_diag$confusion_matrix[1, 2],
         forw_bic = forw_bic_diag$confusion_matrix[1, 2],
         back_bic = back_bic_diag$confusion_matrix[1, 2],
         step_bic = step_bic_diag$confusion_matrix[1, 2]),
  fp = c(forw_aic = forw_aic_diag$confusion_matrix[2, 1],
         back_aic = back_aic_diag$confusion_matrix[2, 1],
         step_aic = step_aic_diag$confusion_matrix[2, 1],
         forw_bic = forw_bic_diag$confusion_matrix[2, 1],
         back_bic = back_bic_diag$confusion_matrix[2, 1],
         step_bic = step_bic_diag$confusion_matrix[2, 1])
)
row.names(logistic_model_results) = c("Forward AIC",
                                "Backward AIC",
                                "Stepwise AIC",
                                "Forward BIC",
                                "Backward BIC",
                                "Stepwise BIC")
logistic_col_names = c("5-Fold CV",
                       "p",
                       "Sensitivity",
                       "Specificity",
                       "False Negatives",
                       "False Positives")
```

```{r}
library(knitr)
kable(logistic_model_results, col.names = logistic_col_names, 
      caption = "Logistic Regression Model Result Summary")
```

Based on our cross-validated misclassification rates, we choose the *Forward AIC* model (note that the *Stepwise AIC* model is equivalent).

\[
\log \bigg(\frac{P[DivWin = 1]}{1 - P[DivWin = 1]} \bigg) = \beta_0 + \beta_W W + \beta_{X2B} X2B + \beta_{HA} HA + \beta_{DP} DP + \beta_{GIDP} GIDP
\]

where the predictors are

- `W` - wins
- `X2B` - doubles
- `HA` - hits against
- `DP` - double plays
- `GIDP` - ground into double plays.

With that, we see that the *Forward BIC* model is fairly close in misclassification, but has half the number of $\beta$ parameters.  Here, the *Forward BIC* model is

\[
\log \bigg(\frac{P[DivWin = 1]}{1 - P[DivWin = 1]} \bigg) = \beta_0 + \beta_W W + \beta_{X2B} X2B
\]

where the predictors are

- `W` - wins
- `X2B` - doubles.

We note that the smaller BIC model is nested within the larger AIC model, so we can perform a Likelihood-Ratio Test to determine if the additional parameters in the AIC model are significant.

```{r}
(forw_likelihood_ratio = anova(forw_bic_model, forw_aic_model, test = "LRT"))
```

If we commandeer the MLR thread's value for $\alpha = 0.05$ and compare it to our test's p-value of `r forw_likelihood_ratio[2, "Pr(>Chi)"]`, we would reject the null hypothesis $H_0 : \beta_{HA} = \beta_{DP} = \beta_{GIDP} = 0$ and declare our preference for the larger forward-search AIC model.

### And the Winning Logistic Regression Model is... ###

The forward-search AIC model.

\[
\log \bigg(\frac{P[DivWin = 1]}{1 - P[DivWin = 1]} \bigg) = \beta_0 + \beta_W W + \beta_{X2B} X2B + \beta_{HA} HA + \beta_{DP} DP + \beta_{GIDP} GIDP
\]

***

## Discussion ##

We choose the forward-search AIC model despite having twice as many predictors as the forward-search BIC model and a near-identical 5-fold cross-validation misclassification rate.  The Likelihood Ratio test comparing the two models tells us that the additional 3 predictors are significant with a p-value of `r forw_likelihood_ratio[2, "Pr(>Chi)"]`.  For our test dataset, we see that this performs well, correctly classifying 80 out of 90 division winners with a cutoff value of 0.5.

```{r}
forw_aic_diag$confusion_matrix
```

The interpretation of our winning model tells us that a team's win total (**`W`**), double tally (**`X2B`**), hits against the team's pitchers (**`HA`**), turned double plays (**`DB`**), and the number of times the team has grounded into double plays (**`GIDP`**) all have have a significant relationship with that team's probability of winning their division.  The relationship with (**`W`**) is fairly self evident and quite boring given the definition of a division winner; the team with the most wins in its division at the end of the season is declared the division champ.  Indeed, this is the most significant $\beta$ parameter by far given the remaining coefficients, with a Wald test p-value of `r summary(forw_aic_model)$coefficients["W", "Pr(>|z|)"]`.

Interestingly, the remaining predictors all have relationships with **`DivWin`** inverse to what we would expect. For instance, there is a somewhat-significant negative relationship with doubles, given the remaining predictors.  That is, given two teams with the same number of wins, hits against, double plays, and GIDPs, the team with the *fewer* number of doubles would have a higher probability of winning its division.  What does this mean?  Well, one explanation could be, given all else is equal, the game favors teams proficient at small ball - being able to manufacture runs with walks, bunts, steals, and sacrifice flies - rather than a team's capacity of lacing ropes to the alley in left-center.

Similarly, we would expect that the number of double plays a team successfully executes wouldn't hinder its odds at winning a division.  But holding the remaining predictors constant, that's exactly what this model says.  So despite the small number of predictors for this model - while it indeed predicts well - it seems it is still tricky to interpret.

***

# Appendices #

## Appendix A - Data Preparation

In the loaddata.R script we load 3 data files "Salaries.csv", Teams.csv and "Batting.csv."  For each, we limited the data to the years of 2000 through 2016.  All 3 dataframes have factor columns that were impacted by the filtering of the data.  This was corrected using the **`droplevels`** function.  We then summed the Salaries data by **`YearID`** and **`TeamID`** and added that as the **`team_salaries`** column.  We performed the same for the Batting data and added that as the **`team_batting`** column.

Next we added calculated columns: 

- `TB` - Total bases
- `SLG` - Slugging percentage
- `OBP` - On-base percentage
- `OPS` - On-base plus slugging percentage
- `WHIP` - Walks and hits per innings pitched
- `BABIP` - Batting average for balls in play
- `RC` - Runs created
- `uBB` - Unintentional walks
- `wOBA` - Weighted on-base average

We then wrote the data out to the CSV file "bbproj.csv."

## Appendix B - Sabermetrics

A decision was made to include a few advanced Sabermetrics in our regressions to determine if there was value in them aggregated at the team level.  Typically, a player's added value, or more precisely, the number of wins a player directly contributes to a team, cannot be established with legacy baseball statistics alone.  Several advanced statistics, called Sabermetrics (loosely named after the *Society for American Baseball Research*, or SABR for short), were developed to correlate a player's on-field statistics to team wins.  Some of them were likely developed using tools described in this course.  

### Total Bases

Total bases (**`TB`**) can be derived from our data using the the following relation:

\[
TB = H + 2B + 2*3B + 3*HR
\]

where:

- `H` is hits
- `2B` is doubles
- `3B` is triples
- `HR` is home runs

Note that in tallying bases, we multiply triples (**`3B`**) by 2 and not 3, etc., since one of the bases is already accounted for in the total hits (**`H`**) tally.  

### Slugging Percentage

Slugging percentage (**`SLG`**) is calculated as the team's total bases (**`TB`**) divided by total at bats (**`AB`**).

\[
SLG = \frac{TB}{AB}
\]

By dividing the total number of bases by at bats, we get a sense of how often a team gets an extra-base hit.  In short, it is a measure of team power.

### On-Base Percentage

We can calculate on-base percentage using the following relation:

\[
OBP = \frac{AB*(H+BB+HBP)+TB*(AB+BB+SF+HBP)}{AB*(AB+BB+SF+HBP)}
\]

where:

- `AB` is at bats
- `BB` is walks
- `HBP` is hit by pitch
- `TB` is total bases
- `SF` is sacrifice flies


### OPS - On-base Plus Slugging

On base percentage (**`OBP`**) is essentially the ratio of the number of times a batter reaches base divided by the number plate appearances. It is a measure of the teams batting efficiency. Slugging percentage (**`SLG`**) is average number of bases that a player gets every time they come to bat, excluding walks. 

\[
OPS = OBP + SLG
\]

Adding the on-base percentage and slugging percentage together, we get a good idea of the team's efficiency and power.

### WHIP - Walks plus Hits per Innings Pitched

Is a derived pitching statistic used to evaluate a pitcher's ability to throw "clean" innings.

\[
WHIP = \frac{BB + H}{IP}
\]

### BABIP - Batting Average on Balls In Play

Is a measure of a hitter's ability to get a hit while keeping the ball in play (i.e. *without* home runs).

\[
BABIP = \frac{H - HR}{AB - SO - HR + SF}
\]

### RC - Runs Created

Is an attempt to directly measure the number of runs a player creates for his team.

\[
RC = \frac{TB(H + BB)}{AB + BB}
\]

[Runs Created](https://www.baseball-reference.com/bullpen/Runs_created) is an advanced statistic created by Bill James in the 1970's.  Interestingly, his statistic aggregated at the team level predicts runs extremely well, despite not having any statistics directly related to runs in the equation.

```{r}
plot(R ~ RC, data = bbproj_trn,
     main = "Team Runs vs Aggregated Runs Created",
     xlab = "Runs Created (RC)",
     ylab = "Team Runs (R)",
     col = "darkgray",
     pch = 20)
abline(a = 0, b = 1, col = "tomato")
```

### wOBA - Weighted On-Base Average

Is similar to on-base percentage (**`OBP`**), giving "extra credit" for extra-base hits, but not as much credit as slugging percentage (**`SLG`**).

\[
wOBA = \frac{0.69 uBB + 0.72 HBP + 0.89 X1B + 1.27 X2B + 1.62 X3B + 2.1 HR}{AB + BB - IBB + SF + HBP}
\]

[wOBA](https://www.fangraphs.com/library/offense/woba/) was developed by Tom Tango in 2006.

## Appendix C - Logistic Regression Models

### Backward Search: BIC Model

```{r ref.label = "glm_back_bic", eval = FALSE}
```

```{r}
summary(back_bic_model)
back_bic_diag
```

### Forward Search: BIC Model

```{r ref.label = "glm_forw_bic", eval = FALSE}
```

```{r}
summary(forw_bic_model)
forw_bic_diag
```

### Stepwise Search: BIC Model

```{r ref.label = "glm_step_bic", eval = FALSE}
```

```{r}
summary(step_bic_model)
step_bic_diag
```

### Backward Search: AIC Model

```{r ref.label = "glm_back_aic", eval = FALSE}
```

```{r}
summary(back_aic_model)
back_aic_diag <- classify_and_diagnose(back_aic_model)
```

### Forward Search: AIC Model

```{r ref.label = "glm_forw_aic", eval = FALSE}
```

```{r}
summary(forw_aic_model)
forw_aic_diag
```

### Stepwise Search: AIC Model

```{r ref.label = "glm_step_aic", eval = FALSE}
```

```{r}
summary(step_aic_model)
step_aic_diag
```

## Appendix D - Logistic Regression Diagnostic Functions

### Cross-Validation Misclassification Rate Function {#cross_validiate}

```{r ref.label = "cross_validate", eval = FALSE}
```

### Classify and Diagnose Function {#classify_and_diagnose}

```{r ref.label = "classify_and_diagnose", eval = FALSE}
```
